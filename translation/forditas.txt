CHAPTERS
  Introduction
  Maps and spatial functions
  Basis for datamodelling on vector based dataset.
  Spagetti model.
  Network model.
  Definition of topology with relational datamodel.
  Spatial indexing.
  
  Basis for datamodelling on raster based dataset.
  Datastructure for raster dataset.
  Intensity transformation.
  Mathematical background on digital filters.
  Digital filters.
  Edge-detection.
  Non-linear filters.
  Kuszoboles.
  LOD algorithms.


Introduction
  An introduction on geoinformatics, a new branch of information technology, using both graphical and database knowledges. In the 1600's mathematics have become the basis for almost every natural sciences, as information technology in our days. Based on high standards, professional softwares, database technologies, graphical representations are used and applied widely among almost every science areas. By keeping the primary goals of geoinformatics,  information technologies have been applying their newest achievements in geoinformation sytstems. New methodologies have been introduced for representing and analyzing spatial based data.
  In the following chapters, concepts, theoratical ideas used by geoinformatics will be introduced. Mathematical methods, principles and algorithms behind softwares are being dicussed in detail. We think it's crucial for students, attending practive oriented classes,that they should get familiar with methods and principles behind geoinformation sysytems, along with their routinish user level and practical problem solvers. 
  Further goals of this material is to introduce a rapidly expanding world of open source technologies. Although only theoratical concepts will be discussed, further materials will rely on these knowledge to introduce problems solved by opensource tools and programming libraries. The world of opensource are relying on modern theoratical, computer science background what we will discuss in the following chapters. Interestingly, the world of opensource are progressing in a much faster pace then commercial softwares, who are essentially aiming for higher profit, thus their software concept only changes when it is inevitable by loosing profit. That's not how opensource world operates, they would rather focusing on technical challenges.
  While editing this material on opensource technologies, bearing in mind that beyond theoratical knowledge, fully functional GIS applications and programming libraries which do not require existing spatial softwares, rather funciontalities are coming from the users by experience.

  What is spatial?
  By first glance the earth is spherical, which leads to the use of polar-coordinate system, having lambda and theta angles with r radius giving any P point coordinate's. (Let's accept this abstract approach, although we know that, the earth is not spherical from projections and referenced surface is not spherical).
  One of the most important task of a map is to express spatiality by representing the earth on a spherical surface. Wether you are using a spatially flat traditional paper map or a digital map on a monitor, both methods needed projection from spherical to planer resulting distortion of the original image. A spherical surface can not be projected to planer surface without undistorted. Projection theory are dealing with similar problems.
  Let us assume that, projected planer maps are avaiable before building a spatial information system. The only exception is data measured by GPS, which are fundamentally geographical coordinates (although transformation is applicable into planer surface).
  Vectorized digital maps have been projected into planer surface by analog source map. Plane to plane transformations are needed when changing projection systems, calibration, georeferencing operations are done. As an example for importing digitalized map, and reproject it into a new projection system, plane to plane transformation is needed.
  In case of remote sensed images, ground control coordinates defines the reference points between relative coordinates of source images and real coordinates of transformed images. In the following sections, we will discuss some of the mostly used planer transformations in geoinformation systems.
  
  Planer transformations
  Put the case that we have a paper map in a given projection systems. Our task is to vectorize it. During vectorization we may keep the source projections system, although we could transform it into new projection system if required.
  Generally raster based digitalized maps are scanned in relative coordinate system (upper-left coordinate is the origin, size are measured in pixels), transformation (georefrencing) is needed into real coorindate system.
  Let's take a look at figure 2., which describes the possible plane transformations like, congruent, isomorph, affine, projective and topologic. For every single transformations invariants are related, which remain constant during transformation.
    Figure 2. Possible planer transformations> 1. Congruent, 2: Isomorph, 3: Affine, 4: Projective, 5: Topological

  > Congurent transformation
  During transformation neither it's size or shape changes.

  > Isomorph transformation
  During isomorph transformation feature's size do change, however it's shape does not. Usually named Conform transformation, due to the angles are not modified during transformation (figure 3.).
    Figure 3. Example for conform transformation: 1: Orientation preserved conform transformation, 2: Inverted orientation conform transformation a) source feature b) transformed feature.

  > Affine transformation
  During affine transformation both it's size and shape are changed, but parallelity is preserved. Note that included angles are not invariants.
    Figure 4. Affine transformation: 1: Original shape, 2: Transformed shape.
  As shown in figure 4. A'B' <> AB and A' angle <> A angle, although A'D' || B'C', additionally every each point on AD lies on A'D'. 
  Equation for affine transformation:
    x' = ax + by
    y' = cx + dy
  The following table summarizes affine transformation's properties: image
  Affine transformation is one of the mostly used transformation method in Geoinformatics. Georeferencing action is applied, when the source map is a scanned raster image, relative coordinates are correlated to referenced coordinates, which is an affine transformation. Most of the coordinate system transformations are based on affine transformation method. 

  > Projective transformation
  During projective transformation none of the following properties are changed: size, shape and parallelity, but straight lines remain straight. This type of transformation is scarcely used on vector based data types. Although projective transformation may come handy for solving representation of 3 dimensional data, where perspective representation is needed.

  > Topological transformation
  During topological transformation angles, distances and parallelity are changed, however, continuity, ordinality and vicinity properties are intact. This type of transformation became relevant when affine transformation did not give correct result, due to the deformation of raster based data during georeferecing.
  Deformation may occur in the course of scanning a raster map, the scanner's paper transfer rollers may slid the paper. In these cases, event a correct georeferecing does not give correct transformation. Despite of correct ground control points and available known points with correct coordinates, which are detectable on the scanned map are not in the correct position. In cases like this, affine transformation is performed location by location, with adding as many ground control points as possible.

  Tesselation
  There are two ways to cover a surface: regular and irregular tesselation. Figure 5 shows the base cases.
  The first case describes the process of segmentation on a paper map. Traditional cartographer's main concern with covering the selected area with paper based maps, which is segmentation.  During this process, they have selected the size of maps to cover the selected area, the type of thematical information to be used, the amount of information to display on the selected paparmap. Based on this standard scales and content have been developed. A section division and enlarged section can be seen in the lower right corner and the corresponded map are shown in figure 6. The surface of Hungary are covered with gap and overlap free in different scale and hierarchially organized sections. The ordinality of each section is tightly related to it's hierarchial position. 
    Figure 6. System of EOV sections, containing the are with 75-342 numbered section. 
    Figure 7. 75-342 map section.
  On figure 5. the second case is a correct tesselation. Information on surface are stored in grids, for example in case of remote sensed or aerial images intensity values on base colours, height values on digital terrain models, the normal value of data are derived from sampled grid data. For a sampled spot (e.g.: pixel) some area is rendered and every data stored within considered as valid data (figure 8.). So, in case of images, these are pixel having the same intensity values or height step with constant height value.
  On figure 5. the third and fourth cases are correct irregular tesselations. The third case cound demonstrates a surface covered by a set polygons with gap and overlap free defined by legal regulations or a scientific fact, like a geological map (figure 10.). Finally the fourth case demonstrates a polygon-system, generated by mathematical, to be exact geometrical algorithm coving the surface gap and overlap freely. Further materials are available on third case in "Vector datamodel", fourth case in "3D graphical, surface modelling" chapters.
    Figure 8.: Sketch of Australia represented with different grid settings.
    Figure 9.: An example for irregular tesselation, where surface is covered according to legal regulations.
  On figure 9. A portion of a cadastral map is shown (only cadastral boundaries), which clearly the usage of irregular coverage, where nor the regularity of sampling and neither the tesselation algorithms can not be observed.
    Figure 10.: Example for irregular tesselation, where the surface is covered by real physical objects (e.g: variant quality stones).


  Chapter 2.
  Maps and spatial functions

    Requirements agains a map can be shortly summarized in few sentences. A map is a model of the actual world, representing the earth surface by a predefined set of rules along with it's properties. We expect the model to guide, navigate, identify our current position spatially and relatively to anchor objects. A model should help us reviewing a larger area, as well as providing all the relevant details on given area with certain context. It should be able to represent an occurence or process, physical quantity, spatial distribution, quality and quantitive measure, leading us to understand and judge a given problem if needed. By comparing multiple objects with their spatial expansion, we should be able to analyze the relation and interaction between these objects. In some cases, a model should provide an overall review on spatial data.
    Both traditional paper maps and modern digital maps fulfill the above listed functionalities. In case of paper based maps, 


  Chapter 3.
  Vector data model

  Vector data model represents geometrical objects with characteristical information like points position, which are (x,y) coordinates for two-dimensional data, (x,y,z) coordinates for three-dimensional data. We should also know and store the projection system definition of coordinates. Supposedly softwares responsible for storing geometrical data can process coordinate systems. 
  Every feature contains a set of points coordinates connected by rules (the simplest case is by incoming order). In accordance with the mathematical summary of chapter graph theory, objects are defined with graphs. Lines and polygons are build-up by characteristical points like vertexes or nodes.
  How to decide which points are characteristical, in order to answer this question, let's take a closer look at an example of data input for vector based data. Data source can be remote sensing imagery or aerial photography images, in order to vectorize input data, we must detect control objects like rivers, roads, buildings and their characteristical points( building's reference points, road's junction points, river's junction points, etc.) can be detected relatively easily. However the substantial part is object dectection, which can be done with human interaction's knowledge, experience and expertise. Without him/her vectorization process can not be delivered.
  Another consideration aspect is based on the purporse of usage for vectorized map.  According to predefined purpose of usage, the list of important, less important and neglectable objects are known, before vectorization process. For example, in case of vectorizing administration maps based on scanned paper based maps. In this case, every administrative boundary are important data, on the contrary rivers are less important, although not neglectable. Streams can be neglected, therefore are not present on digitalized map (even if it was present on the paper based map).
  Figure 20: On the right handside, the reality, which will be stored and represented in vector data modell.
  Vector based representation as described in figure 20, objects are projected perpendicularly to the surface in a coordinate system which is scaled in North-South orientation. During the process of vectorization based on paper maps, human interaction is required, with expertise in order to detect and select measure and characteristic points. At the end, our conclusion is that, human interaction with vast background-knowledge is required in the process of vectorization.
  Figure 21: (left-hand-side) Examples for one-dimensional objects: a) line segment (section); b) linestring (polyline) with extreme points (end points) and vertices; c) non-intersected linestring; d) closed polyline; e) monoton polyline; f) non-monoton polyline; 
  Figure 21: (right-hand-side) Examples for two-dimensional objects: aa) simple polygon; bb) complex-polygon (only exists as invalid object in geoinformatics); cc) convex-polygon; dd) monoton polygon; ee) polygon with inner structure (hollow polygon); ff) region with geometrically non-related objects [Rigaux et al]
  
  As represented in figure 20, where the real objects are vectorized. For modelling real world objects, geometry elements are used, like 0-dimension point and 1-dimension line. A specialized case for lines are polylines, which is a sequence of connected lines (by connecting lines, a constraint have been stated on vector data model, by introducing topological relation between lines). The other geometrical object is a two-dimensional polygon, build-up by a sequence of points (figure 21). A more accurate definition for a polygon is a sequence of connected segments, having identical start and end point, moreover it's topmost attribute is that, it's a closed area. Connectivity and closeness are topological constraints on connected segments. 
  Figure 22: How Complex features are built-up from simple features (geometric primitives): 1 -- level of nodes; 2 -- level of lines and polygons; 3 -- level of features; 4 -- levle of complex features.

  As represented on figure 22, only on base level, nodes contains coordinates data. Other features higher than node level, like linestring, and polygons contains only structural information, describing information with related features on lower abstraction level, which means that storing geometrical data is not sufficient, therefore creation and relation data between hierarchy levels are also required, for example features on lines and polygons level, should contain corresponding data on sequence of related points creating lines or polygons. Vector data model has various advantages, like:
  - nodes are stored when there is break point on a segment.
  - hard disk space efficient graphical representation.
  - an option for set up feature hierarchy levels.
  - unlimited number of connections to databases.
Figure 23 describes data stored in database. Additional information, attributes can be added in form of database tables, if business logic requires so. 
  Figure 23. Schematic diagram on relation between graphical and descriptive data. Relation between graphical objects and descriptive data is 1:1.
  Figure 24. By creating feature groups yields adaptive data structures. Object-oriented system name it as "feature-class" (attribute classes), classical based on CAD terminologies software developers rather call it as "layer". As an example a feature class have been described: real estate values, containing parcels, buildings and public utility classes.
  
  For the ease of handling, vector data should be grouped logically depending on our purpose. Arbitary grouping based on declartion of system designer,developer could be a grouping method, where a designer's job to sort objects into groups (feature class or layer) eventually with greater resposibility over the system. Data can be groupped by feature attributes. Groupping can be fixed with non-variable data or flexible with dynamic grouping conditions.


  Chapter 4
  Spaghetti model
  Vector data are stored by coordinates, as well as the relation between features, moreover connection relation should also be stored in vector model. Many everyday questions reveals the importance of topological data structure:
  - "How do I get to the train-station", asked by a lost touriest.
  - "Go straight ahead until the second turning, then turn right and follow until it crosses a level crossing, after that keep left to the tracks you will reach the station".
  - "What's in between my and my neighbour's parcel?". The answer would be: "Nothing, only a fence, which is a 1-dimensional object", otherwise we could not be neighbours if there is another object between us.
  Topology describes relations and connections (overlap, break points) between features.

  Spaghetti model stores only connection rules between nodes and coordinates refering to nodes. It does not investigate topological relations between objects, for example neighbours of a polygon, is there a common node for overlapping polygons. It also does not store continuity and the order of features.
  Figure 25. On of the typical mistake in spaghetti model. Dotted and continous lines should rely on each other, however it's not true, as shown by arrows, moreover the arrow with a question mark reveals a node which does not have a corresponding continous pair.
  Most of the GIS softwares provides a general editing tool based on spaghetti model. It's advantage reside in simplicity, at the same time also a disadvantage. By letting a loosely coupled data structure in use, many mistakes are made, resulting bad quality spatial databases. GIS experts are capable of producing high-quality spatial data, although it's not advisable to apply spaghetti model in a widely used environment. General editing tools are using their own data format, which does not support relational data structure design elements like vicinity and relational considerations.
  Despite of error proneness by using spaghetti model, some of the GIS software are still using them. For checking if a polygon a ring, by inspecting discontinuity of linestrings is done on software level, even though data structure does not support data consistency which may be checked by comparing coordinates by coordinates. As shown in figure 25 a concept for modelling spaghetti model with relation database. The "geometriai elemek" table store unique identifier on nodes (primary key) and their coorindates. The "objektumok" table contains the unique identifier of additional attribute data, like "Name", "Field1", etc. Note that, the order of nodes is crucial, since only incrementally drawing nodes yields correct polygon. Structure described in figure 25 gives the ability of storing polygons topologically, however with no guarantee that it's correct all the time. Manual or software based interaction is needed for correct topology.
  Figure 26. Nodes and polygons composed of nodes defined by connection tables. However it does not contain any information on topology.
 

  Chapter 5
  Network model
  It was intended to define objects by a network of segments with network topology like transportation orgranization, shipping, logictical challenges, public utility networks and relations. All of them  are modelled based on graphs where nodes are defined by business logics rules. Arcs and polylines are distinguished.
  As you can see, it's recommended to handle polylines containing many nodes as arcs, especially in routing optimization questions, because these polylines does not have junction points. Many graph algorithms exists for routing questions, however due to the content-lenght limitations of this book, routing optimization will be left out of discussion.
  Figure 27. Nodes, polylines and arcs composes networks [Rigaux et al]

  Linear referencing
  Localization can be determined in various ways. The most common way by giving x,y,z coordinates along with postal codes, which have been applied for quite a long time. (Let us asume that a friend of us, instead of  giving the address, coordinates are given, we would be surprised, as postal code based localization method is known and accepted.) Line based features, like public utility and transportation networks (roads, railways),for describing any points on it, let's use the following method, which is one of the most commonly used model based on network model. For example kilometer stones on roads and riverways, each stone includes it's exact coordinates, however in many cases localization depends on the location of these stones (eg.: 150 meter from the 123-rd kilometer stone along with the downstream. Usually relative localization are being used for road incidents, like "accident occured on route 4 between 52nd and 53rd kilometer stone. Kilometer stones on rivers and roads like break points on map segments, can be considered as points.
  Line segments representing edges in a network model, are not always resides on break points (a 10km long straight railroad segment, which has only one start and end point, no mid points). It's unnecessary to store nodes on a straight line segment. By storing nodes on each break point in map segments a linestring can be genrated from it, however it's not correct because we would introduce many redundant points in the geometry (larger, multiple level junction points are stored on each path's crossing points, or a river have been redirected during a construction of a power plant or a dam). Significant length increase may occur, if we keep the segment based logic, resulting modifications on kilometer based segments, which is not possible to do so.
  Eventually no matter what method are being used for localization, in case they are congruent and leading to the same result. Methods may vary depending on application areas. A few of these localization methods are: geographical coordinates; road segment number; relative distance to segment identifier; relative distance to town border; relative distance from kilometer stones; distance from junction points; distance from significant points. Each application area are building their information systems on different scale levels depending on business logic corresponding to their field of interest. By using different scales and satisfying business logic needs, segmentation is a crucial problem as shown in figure 28.
  Figure 28. Schematic representation of linear referencing applied on different scales and business logic.
  Figure 29. A linestring represented by nodes (n1,n2,n3 and n4) matched with segment break-points.
  On Figure 29 a Linestring represented by n1,n2,n3 and n4 nodes coordinates and by linear referencing (segmentation) on the same nodes (e.g.: distance from a given segment number, as shown in the following table).
tetszeni  Table 2. Georeference table of nodes n1,n2,n3 and n4.
  Table 3. Linear reference table of nodes n1,n2,n3 and n4.

  Dynamic segmentation
  Layer based approach are segmenting graphical data, and applies business logic on segments which are part of the graphical database. Due to the popularity of object oriented methodology, dynamic segmentation may be applied on any attribute of a graphical data, resulting a more flexible way of building and designing systems. Especially in linear referencing systems, where segments are implemeted by linear referencing as shown in Table 4 and Figure 30.
  Table 4. Linear referencing have been applied resulted in symbol K.
  Figure 30. A Linestring, with segment break points and K symbol linestring, defined by linear referencing as defined in Table 4.


  Chapter 6.
  Define topology using relational tables

  Figure 31. Points, contours, polygons and countries named relational tables [Rigaux at al]

  In order to implement a vector data model, it's natural to use relational data-structure. In case of spaghetti model, as we have experienced that polygon structures can be well defined by using relational data structure. Now, let's examine, how to refine relational structure to achieve topologically correct structure automatically. 
  Let's specify the above mentioned example, by defining contries in Europe as additional data (attributes). As described in Figure 31. basic geometries are stored in "pontok" table, "konturok" table containing contours is referencing to "pontok" table, "poligonok" table containing polygons is referencing to "konturok" table. "Konturok" table contains as many linestrings as borderlines (each neighbour polygons) each polygon (surface) has, at last "Orszagok" table references to "poligonok" table geometrical data, other additional data like (name, capital, population) are stored inside table structure. Notice that in case of modifying a node (eg.shifting) does not ruin the topological structure at all. An efficient structure like this ensures the topological properties, despite of it's topological structure, we are still calling it a relational model.
  For example let's take a look at an SQL command selecting contour lines of France:
SELECT poligonok.id-contour 
  FROM országok, poligonok, kontúrok, pontok WHERE name=`France`
  AND országok.id-boundary=poligonok.id-boundary
  AND poligonok.id-contour=kontúrok.id-contour
  AND kontúrok.id-point=pontok.id-point
  ORDER BY poligonok.id-contour, point-num

  Chapter 7.
  Polygon topology described by graphs.

  As described topological storage consist of points, arcs and polygons. The major question is: how can we store objects with their relative relational information with each other like neighbourhood integrated into data-structure. By using relational model, the correctness of topological structure is not self-maintained (using softwares, which enforces the topological correctness from users, avoiding topological errors).
  A topological data-structure must contain the following steps:
    - storing the topological relationship of geometrical elements.
    - enforcing correctness on data, like avoiding gaps between polygons (exception can be city borders), or intersection, etc.
    - storing embeded polygon relations, may occur quite frequently on maps (eg.: an island in a lake).
    - storing additional information on geometry data: colour of a polygon, linestring is represented as dashed lines, etc...
    - able to answer topological queries like: location of a point, what objects does a polyline intersects, neighbours of a polygon, embeded polygons, etc.
  Geometry types are point, linestring and polygon, therefore layers can be defined according to their type. As mentioned above, it's sufficient to store a unique identifier and coordinates of a point. A point may represent a standalone entity of an information system, not only representing an element of a compound geometrical object. In case of independent entities, grpahical representation of points are carrying thematical content. Graphical representation usually are described by a predefined graphical symbol, which is selected from a symbol tables or  collection of representations.
  It's practical to describe polylines with graphs, where vertices are points forming linestrings, arcs are representing connection between each nodes. Graphical styles can be assigned to each linestrings (bold, dashed, multi-parallel lines), although the graphical representation does not touch the position of nodes forming linestrings. 
  Polygons are represented by compound structured graph, namely simple graph. A simple graph consists of three element types: area, edge and junction point. An area represents the actual area of polygons, edges are representing the borderlines between polygons, where junction points are representing the crossing points of polylines. (as described by relational tables on figure 31).
  
  SIMPLE Graphs
  Planer structure are stored in two graphs. One of the graph stores the junction points, between junction points are edges represented by polylines. Edges are also storing which areas are on the right and left handside relatively. Let's call it a simple graph (Figure 32.)
    Figure 32. An example for simple graph. On the left handside polygons are displayed, on the right handside areas defined by simple graph.
    Figure 33. Dual graph of the simple graph displayed in figure 32. and extended by inclusion areas for the dual graph.
  The other graph is the dual of the simple graph, where nodes are vertices, edges are representing neighbour relation of an area. An area contains it's surrounding polylines, namely the polygon itself representing the area. One of the purpose of a dual graph along with simple graph is to represent the topological relation between polygons, and it's geometries. Neighbourhood, reachability questions can be easily answered (however, with current structure, we can not store embeded areas).
  Let's define beside real areas a new type, namely inclusion areas. These areas however can not represented as separate polygons in reality. It's an inclusion area, a contour for a set of polygons, where every element can be reached with a simple graph on it's dual graph (Figure 33.). The outer most inclusion area, which is the contour of polygon-structure, namely, the upper most level of an area, whereas on other levels a whole with a real polygon inside.
  INCLUSION Ggraph
  Real and inclusion graphs can be ordered in hierarchy, and represented with graphs. A root element is the image's contour denoted by a bounding box, it's children are those polygons, which can be reached by the root element in dual graph. Hence, for each real area we have defined a bounding box, on the other hand, beside of the image's bounding box, every bounding box, contains at least a real area.
    Figure 34. A real and bounding are with contour style. Real areas a noted with letters, where inclusion areas are noted by numbers. An inclusion graph is represented on the right handside of figure.
  Within an inclusion tree,  the contour of the whole area is represented in the root node. On even ordinality levels, inclusion contours are displayed whereas odd ordinality levels real areas are displayed. An inclusion area may have many children, which can be reached by dual graph traversal algorithms. An inclusion graph is not stored on a separate datastructure, it's been extended by dual graph, where spatial relations are stored. Let's take a look at figure 34, where C polygon area contains subpolyons included, which are areas (D and E), therefore C contour is the inclusion area corresponding to D and E polyons. An inclusion area is represented in the dual graph, in such a way, that each of it's neighbour polygons are located on the edge of the included set of polygons. For each area, we also store wether it's an inclusion area or real area.
  In case of real areas, they also store the list of edges with negative direction of traversal. In case of an inclusion area, directed border edges with pozitive traverse order and the list of included areas are stored. In a simple graph, edges on the verge of an area, have area on one side, however, data related to included areas will be set. An edge on a simple graph and dual graph are identical in a way, that a edges of a simple graph are representing a border between two different areas in a dual graph.
  Since edges are identical in simple and dual graph, therefore duplication is not allowed. In a dual graph, an edges are inserted in a way that only it's left and right neighbour areas stored. This method leads to simple usage when changing from simple to dual graph, on the other hand, changes made on a linestring are presented on both graph, which leads to correctness of topological order, errors are avoided naturally. Dual graph representation are resulting topological datastructure.
  On a simple graph, edges are non-directed, although both directions are stored over an edges, due to the fact that, it's easier to build a polygon, when both directions are present. Points are represented by coordinates and outgoing edges in clockwise order. By default edges do not store their direction, however we can add directions to them, where the start and end points identifiers, as well as left and right area's identifiers are stored. If left or right areas are missing, the identifier of contour area is being used. In addition the polyline related to the edge is stored as well.
  The advantage of this data structure that it supports all the major functionalities and topologically correct. In the next section we will discuss the preparation of this data structure.

  SIMPLE GRAPH and it's Dual
  A pre-condition for building a simple graph is a topologically correct set of polygons. End points of linestrings are collected in the first step, from end points simple graph's nodes are created and stored, identifying points on the surface. Linestrings are represented by edges. Edges are inserted in edges list preserving the correct order.
  After simple graph have been created, we can create it's dual graph. First of all, areas are created by processing edges, which are processed in an ordered manner e1,e2,...,en. At step i the following task must be done as described in Figure 35:
    - check the existance of areas on both sides of edge ei.
    - if yes, then move on to the next edge.
    - if not, then:
      a. new T area is rendered on the right side of edge ei
      b. travel around area T, that T is on the left-side of edge ei with respect to direction of traverse that ei is the first edge in the list.
      c. traverse order on points should keep the order in edges list.
      d. on travesed edges, let's set T area on the left handside of the current edge.
      e. at the end of traverse, resulted contour polyline should be stored, actually this is the polygon called T.

  Figure 35. Original set of polygons; direction of first step; steps in creation of simple graphs; final simple graph with dual graph areas; nodes and edges of dual graph (darkened areas represents real areas, where lightish areas are included areas).
  
  The algorithm of building areas defines areas and their relations, in addition polygons are determined, which encloses these areas. From the direction of traverse are determining wether an area is real or included. If nodes of a polygon is stored in the order of traverse in the contour polygon, then it's a real area, otherwise an included area. This polygon can be checked by signed area calculation. When all the inclusion area have been determined, there's not much to do, than the construction of the inclusion graph, which can be done the following way:
  - let's order areas in descending order by their precise area.
  - the first inclusion area which is the whole picture. Let's perform a search in the tree(melyik faban, simple vagy dualis?) that which inclusion area falls into a real area, as a result insert these areas into the dual graph's node representing this area.
  - by performing a traverse on the dual graph from the actual inclusion area, reachable areas are inserted into dual graph's node representing the inclusion area.
  Due to ordering, cross inclusion problem have been eliminated where an included area contains it's inclusion area as included area. The given datastructure has many advantages like:
  - No duplication storage for edges, therefore topological correctness is true all the time. For example a linestring's position  have been moved into intersection with another linestring. When a linestring is altered, it will effect both polygons, which are separated by this linestring.
  - It's easy to query a polygon's neighbours, since the dual graph will give the neighbours of current polygon.
  - Support for multi-level embeded areas and the containing relation can be easily investigated by using inclusion graph.

  ALTERING SIMPLE GRAPH
  Beside building datastructure, we should also examine in cases of modification is made on data. It's relatively easy to remove an edges, we must check which areas have been separated by the removal edge. In case of real areas, we must combine these areas into one new area. In case of real and inclusion area, real area will be attached to inclusion area, in a way that the neighbours of the real area from now on will be set to inclusion area via new edges of inclusion area. After this, we should also check if there is any node having only two edges connected to it. It there is, remove them and join the two edges in the neighbour's list.
  Figure 36. An axample for altering a simple graph by inserting a new polygon (middle) into the graph with creating and deleting new nodes and edges. It's more convenient to build a new graph than altering the original.
  A major disadvantages of simple graph occurs when we want to insert a new node. Multiple edge will be introduced into dual graph, therefore the insertation of a single node should be prohibited unless it comes with the insertation of edges, resulting new areas. Inser a polygon with caution, because rebuild of simple graph is required (figure 36).

  SPATIAL INDEXING
  Digital maps, vector or raster based usually requires large amount of processing and memory resources. During graphical representation of digital data, a bidirectional correspondence is made between database and display device.
  Figure 37. Scale representation of the actual proportion between a viewport and database.
  Grey area described on figure 37 represent the whole area stored in database, whitely coloured area represent the actual viewport displaying desired area. The most straightforward way is to read the whole database into memory, then initiate the graphical drawing of data, however this method is not convenient in many aspects. On the one hand, it's unnecessary to read data outside the viewport, on the other hand, memory usage is limited, therefore it's practical to load data inside the viewport. Moreover, by reading data from harddisk is much slower than from random access memory, therefore it's practical to split-up data into pages, and read them page-by-page. An I/O operation is equivalent with a read/write of a page. Then again, the number of reads must be synchronized with the size of a viewport. Both of the above described conditions represent opposite interests, which can only be resolved by indexing of data, reducing the query against database on different viewports.
  For rapid querying on data inside a viewport, we must exclude the rest of data. Unfortunately the traditional indexing methods like B-tree on relational databases do not support this type of indexing requirements. A solution for it are spatial indexing algorithms. Before discussing the spatial indexing algorithms, let's get familiar with terms like centroid and bounding-box.
  Figure 38. Centroid of a convex and concave polygons.
  A centroid is a replacement object, a point, which is inside a polygon and capable of representing a polygon's spatial orientation. It's obvious that in case of convex polygons a centroid is it's center of gravity, as for concave it's a point on the centroid line (as represented on figure 38). A bounding box is the quad with smallest area containing the actual polygon (as represented on figure 39).
  Figure 39. A bounding box.
  Both terms are valid for polygons and polylines. During spatial indexing these terms are needed to represent remarkably complex objects (polygons and polylines).
  For indexing spatial data, there are two methods for it, namely spatial objects partitioning or split-up the examined space by partitions. In the following we will discuss grid indexing and quad-tree indexing methods, which can be classified as space partition algorithms for accelerate the query. Lastly R-tree indexing algorithm will be discuess, which is a data driven indexing structure.

  GRID INDEXING.
  Let's split planer objects into regular grids as described on figure 40.
  Figure 40. Objects with different areas displayed on regular grids.
  In order to search objects rapidly, the first step is primary filtering by using coordinates of objects and sort them into grids, where grids have their own identifier index for faster querying. Objects falls into given grids are returned as query results. By index indentifiers we are requesting objects related to that index, which is more efficient than reading the whole database (Figure 41). Secondary filter does the processing of geometries on found objects (e.g.: in case of polygons reading of vertices and displaying them).
  Figure 41. Index table and it's relation to the original table.
  As described on figure 40, the size of grids are not always proportion with the size of objects. If the grid size is too large, then too many objects are related to it, which leads to inefficiency in case of extreme data. If grid size is too small, then large objects may relate to too many grids, which also not very efficient. The algorithm may improve if we use altering sized grid levels.
  Grid indexing algorithm is a simple and efficient way of spatial indexing. Many commercial softwares are applying this type of indexing, despite of it's shortcomings, especially when the spatial distribution of objects are not equidistributed. The optimistic scenario would be that every grid should contain closely the same number of related objects, however due to the constant size of grids will not make it possible. In case of inhomogenous spatial distribution leads to the inefficient of secondary filtering, therefore we need new indexing algorithms with better conditions. For homogenous data distribution like raster based data structures, grid indexing can be applied. Although quadtrees indexing algorithm is much more efficient and widely used.

  QUADTREE
  Quadtree is a hierarchial data structure, based on the principle of "devide and rule" using recursive decomposition. There are many variations of quadtree implementations, where point region or PR-quadtree will be discussed. The basic idea behind this is to devide the space into four/quad planes (figure 42) called space-driven structure.
  Figure 42. A point and a quadtree plane division.
  A tree-structure is used for indexing structure, where every inner-node represents a plane range, in it's leave objects are attached. Root of the tree is the starting range. Every inner-node contains it's own partition's centroid and four pointers on four orientation partitions with abbreviation according to quarters namely: NW (north-west), NE (north-east), SE (south-east), SW (south-west). If a node has no child than it's a leaf, the flag related to it will be set as true. A leaf can be empty or contains object with coordinates and additional attributes. Orientations of a leaf are undefined. Note that in case of inner-nodes it's not necessary to store orientation partitions because partitioning is always trivial. Due to the trivial partitioning, with a correct tree data-structure implementation, storage of partitions can be avoided.

  Figure 43. describes a quadtree containing seven objects with their coordinates.
  Figure 43. Recursive plane partitioning and partitioned levels in quadtree structure.
  
  A quadtree is build from it's root towards the leaves. During traverse, the child of given inner-node will be selected by the inserted point's coordinates. If its a leaf and empty, the insert it. If the leaf has already contains an object, then the range should be re-partitioned until the old node and new node are not in the same range. This type of partitioning results in many new partitions in case of relatively nearby points. Partitions can be eliminated during point deletion. When a point is deleted, and the containing partition has only a point, the partitions can be merged together.
  Indexing can be implemented using a quadtree structure where partitions are equivalent with page size, namely plane range should only partitioned when points stored in partition are overflowed in page size. With this modification leaves does not contain only one object, but a set of objects, which will be stored in the same page.

  In case of polygons the algorithm is not that simple. A complex object like a polygon has many points, therefore as indexing method, we are using their centroid or bounding-box. Both terms centroid and bounding-box are suitable for determining the polygon's location on a quandrant. A centroid can only and only belongs to a single quadant, on the other hand a bounding-box may belongs to many quadrant, as described on figure 44.
  Figure 44. Quadtree and polygons bounding-boxes.

  Up to this point we have only discussed quadtree algorithm for vector data model, however it can also be applied to raster based data model, due to the fact that the distribution of raster data are equidistributed. As for raster based data model, partitioned quadtree shall be used. The structure of it is similar to point-based quadtree, the only difference is that leaves contains all the information on the range sub-partition. This quadtree can be used for compressing images as follows: execute partitioning until the sub-partition contains on homogenous data, then intensity values will be stored in leaves.

  R-TREE
  The above discussed algorithms apply partitioning on the plane, irrespectively of objects spatial distribution. Disadvantages of this type of algorithms revealed when distribution of spatial objects are uneven, for example in case of vector data models. R-tree is data-driven tree structure. Many versions of r-tree algorithms are present, however we will discuss with the basic version. R-tree is is a hierarchially embeded or overlapped system of bounding-boxes. Each node of the tree is equivalent to a page table on the disk (dataset read from disk with one cost). A node may contain leaves or further nodes. Nodes containing sub-nodes are called directory nodes (the name suggest that bounding-boxes may contain sub-nodes or leaves). A leave contains coordinates of the bounding-box (not only 2D bounding-boxes) and an indentifier. The root of the tree is R  containing at least two nodes.
  Figure 45. An R-tree. Bounding-boxes a,b,c,d contain further sub-nodes, which can be nodes or leaves and so on.

  Figure 45. demonstrates a simple case where bounding boxes signed with a,b,c,d letters contains further nodes.
  Figure 46. Point search performed on R-tree. As you have noticed that point P is in the directory of c and d bounding boxes and not in a and b directories, therefore we only search in c and d directories recursively. At the end we get that P point is inside leaves with identifier of 8 and 12 bounding-boxes. The simples search method is point based which can be executed in two steps. First step is to look up all directories may contain the given point P under root node (don't forget that bounding-boxes may overlap). In the second step, let's examine directories found in first step that which of them contains leave with the given P point in it (this can be recursive). Based on point search, linestring and polygon search are executed in the same manner.

  Chapter 9.

  BASICS OF RASTER DATA MODEL.
  Raster based data model is the other major data model of geoinformation technologies. The major difference between vector and raster based data models are in the data structure and the projection of surface objects. The level of details of surface data are significantly different along with data processing methodologies. For instance vector data model are based on relation databases, where raster data models heavily rely on different image processing algorithms. These algorithms rely on mathematical methods like Fourier transformation or multivariable statistical knowledge like cluster analysis or main-component-analysis.

  ELECTROMAGNETIC SPECTRUM
  Figure 47. Valuable range of electromagnetic spectrum. RGB define the visible range, where near IR and middle IR and far IR are infra-red ranges.

  As described on figure 47 electromagnetic ranges are used in remote sensing. Raster based GIS (geoinformation systems) used these ranges. Light emitted from the sun and reflected from the surface of the earth are sensed and stored in image format which are stored on harddisks. Let's examine the method of specifying a digital image.

  PIXEL, SPECTRAL RESOLUTION, WAVELENGTH
  A digital image's most basic building blocks are pixels, the smallest imagepoint, which can be created by image creation tools. An image creation tool can be satelites, scanners or a digital camera. A pixel optically is homogenous, namely it's colour and photometrically are the similar. The captured area in pixels have real measurements, namely scale.
  The term spatial resolution is closely related to the size of pixels and the size of the capture image. The resolution is proportionally equivalent with the number of pixels it contains on a captured area. For example an image with the resolution of n*m pixels (n rows and m columns) on a smaller area is considered as a high resolution image, however if the captured area is vast then the resolution is quite small, although in physical size it does not change at all.
  We must also know the spectral range where the image taken (with a digital camera). Simple digital cameras are working in the spectral range of visible lights, where other devices are working in the spectral range of infra-red. In the era of satellites for sensing different spectral ranges tool have been developed. Some of them are working in visible spectral ranges like SPOT and ICONOS, where others are operating in a wide spectral range like LANDSAT TM, which operates on RGB and four infra-red bands. A device spectral resolution number is proportion to the number of bands it can sense. The most advanced devices are hyperspectral cameras, which can process hundreds of electromagnetic spectrals into bands. There are also many devices operating in radio frequency ranges, based on radar concept for sensing surfaces. The major difference between radar and the above mentioned devices that, radar based devices emits waves to detect and sense data, where others are using emittions from the sun.

  COLOUR DEPTH AND TRANSPARENCY.
  The representation of colours have evolved with the development of computers. In the beginning colours were represented on 4 later on 8 bits. Coloured images were poor in tones, it's rather suitable for presenting as graphic. Nowadays colours are represented in 24, moreover in 36 bits on computers, therefore digital images are capable of displaying millions of tones. Figure 48 describes the bits with representable colours.
  Figure 48. Colours by pixel in numeric representation.

  Spatial images are using the above mentioned colour representations, moreover during preprocessing states algorithms usually transforms images from one model to another (e.g.: thematic representation). It can't be overemphasized that, a black and white image represented in 1 bit is not equivalent with the black-and-white image we know of, namely 256 (8bits) greyscale represents objects projected tonal conditions, while on 1 bit representation model only 




  COLOUR MODELS
  Let's take a three dimensional coordinate system where the axis represents the colours. 
  Figure 49. RGB colour cube.
  The RGB colour model is a composite of Red, Green and Blue colours, which have been created by:
    Colour = a*Red + b*Green + c*Blue
  where a,b and c coefficients stands for the ratio of RGB components in the resulted colour. By applying the 24 bit based colour representation mode, we will get 2 on the power 24 shades of colours, which is 2^8 of Red, Green and Blue shades can be represented, namely 256 intesity value can be set for each base colours.
  
  HSI COLOUR MODEL
  HSI colour model fundamentally different from RGB model. Whereas RGB usses Decarte's coordinate system, HSI uses polar-coordinate system for representing colours as described on figure 50.
  Figure 50. HSI colour cone.
  HSI is the abbriviation of Hue, Saturation and Intesity words. Remote sensing softwares support both colour models. There are many more colour models like CMYK (mostly used for publishing areas, printing industry although these kind of models are not used in remote sensing.
  
  RASTER DATA-STRCUTURES
  Raster data-structures are simpler than vector based, since their common property is a table, each row and column contains information on a pixel which can be intesity, height values, etc... Let's take a look at figure 51, where a pancromatic (grayscaled) image have been described in the above mentioned table structure.
  Figure 51. Pancromatic (grayscaled) Image table representation.
  There are many raster file formats present, but most of them are based on table structure, therefore we will not discuss in detail these file formats. Figure 51 descibes a schematic representation of a raster image, because most of the software companies dealing with remote sensing are working on the same basis with their own modifications on the method of transforming into their own data format. This knowledge will be gained when conrete usage of these  softwares are required. Data table on figure 51 contains only one intensity band, however in case of RGB based image, there will be three data tables (intensity matrix) representing the image. In case of 32bits images, a fourth intensity matrix is also required to described the transparency data.
  
  In case of multi-spectral remote sensed images along with RGB bands, each infrared bands are having their own intensity matrix. By taking into consideration the speed of development around hyperspectral remote sensing, an image may contain hundreds of intensity matrixes. It's quite a challenge to handle the large amount of data, not to mention the interpretation of each band's data, therefore we may state that, remote sensing is an enormous research area waiting to be exploited.
  
  WELL-KNOWN RASTER DATA FORMATS
  Many raster based data formats are used in every day applications, as well as in remote sensing. Some of  the formats are closely related to operating systems (eg. bmp) or compressed images created by lossfull (jpg and some of tiff formats) and lossless (tif, gif, png or jpg2000)  algorithms ,while others are not compressed. A few data formats will be described, since many books are discussing them thoroughly.
  TIFF. Used for general purposes, especially applied in progressional image processing areas. Platform indenpendent. Widely spread and used among remote sensing, pixel based data representation are capable of storing image parameters, text and numeric based data along with pixel data. It supports both shallow and professional 24 or 32 bit colour models, until CMYK colour models used in printing industry. Compressed (lossly and lossless) and uncompressed formats are available. A variation of it namely GeoTiff contains georeferencing control points, therefore by reading it into a remote sensing system a georeferenced data will be available immediately.
  
  Jpg: An outstandingly efficient compressing algorithm have been used, by keeping rich details of images. It's widely used by digital photography. The amount of quality loss can be calibrated by the end-user. Of course, for larger compression, the more image quality loss is produced. It's only used for storing rich detailed data, like photos. It's not suitable for compressing line-based raster formats, because the quality loss is significant. A georeferenced variation of it is Jpg2000 which is a lossless format, although haven't been used widely.
  
  Bil: Specifically created for storing remote sensing images in binary format, capable of storing frequenc data bands, calibration points, corner coordinates and projection system parameters. The *.bil file contains intensity value for each pixel by band, *.hdr header file contains attributes data, like georeferenced points, corner and column data. Widely spread and used by many remote sensing softwares, systems, moreover, vector based systems capable of processing raster data are also supporting this type of raster format.
  
  Chapter 10. INTENSITY TRANSFORMATION
  Histogram equalization

  All images contains different photometric values on each pixel. These pixel photometric values are varying from black to white. Although, this never occurs in real life. Due to the environmental and digital projection methods, the dynamic range of intensity values are much narrower than expected.
  Figure 52. On the left a histogram of Band A, which is the frequency of intensity values on pixels. On the right Band A and B with crossplotted intensity space, describing their relationship.
  
  However all images contains a darkest and brightest pixel, but the brightest and darkest points are not equal to theoratical white and black points in RGB colour model. By representing the light emittion values and their occurrence frequency in a coordinate system resulting a histogram, which is the basic of image processing methods.
  Figure 53. Process of histogram equalization: histogram of the original image(upper left figure), histogram after transformation (upper right figure) and transformation function (lower figure).

  Histogram is the probability (P) representing the occurrency of intensity values on pixels, by calculating the occurrence of an intensity value (i) on every point:

  Minimum and maximum intensity values of an image can be determined by a histogram. During histogram transformation, the transformation function will transform the intesity of pixels into the minimum and maximum range, so the histogram have been streched by increasing the difference between intensity values, which is the contrast.
  Many histogram transformation methods exists along with the linear transformation. Any function can be applied for intensity transformation, as described on Figure 54. The intensity value of point i, can be given with the following formula:

  applying for on an image matrix with 24 bit RGB colour model by the following function:

  where f(x,y) represents the original image on (x,y) position, min and max are the minimum and maximum intensity values on the original image, g(x,y) transformation function will transform the image into an intensity range between 0-255. We have increased the intensity difference on pixels, hence increasing the contrast of the image. This transformation method is historgram equalization.
  Figure 54. Intesity transformation can be given by any T function.

  Figure 55. Original SPOT image.
  Let's take a closer look at Figure 55., which is a black-and-white image representing a remote sense image. Let's create a histogram for it, representing the probability density function of intensity values varying from 0-255. As you can see the darkest and brightest points of the image are not 0 and 255. Usually the distributed function which is the integral of histogram is represented instead of the histogram.
  Let's transform the original image by increasing the dynamic range of the image, namely the darkest point of the image is 0, and brightest point is 255. The result is shown on Figure 57.
  Figure 56. Histogram and distribution function of the remote sense image. Indicating the possible dynamic range is quite narrow, because the darkest point starting somewhere at 60, while the brightest point is around 120.
  Figure 57. Remote sense image transformed by histogram. Contrast have been significantly increased, as the brightest area on the image is almost white (255), where darkest areas are almost black (0).

  Let's take a coloured image and apply the same histogram equalization transformation method on each RGB bands.
  Figure 58. LANDSAT image.

  Figure 59. LANDSAT image after histogram equalization.
  If histogram equalization is applied on each colour, it may result changes on aspect, colour characteristics, rations, appearance of the image. However these changes are not relevant on remote sense images, at the same time, its curcial to keep the colour characteristics of an normal image. Figure 59 shows the LANDSAT image after histogram equalization.
  The role of base colours are emphasized by increasing their intensity in between 0 and 255 with transformation, therefore colours of a raw image may gain some artificial characteristics, namely we may break up the colour balance of the image. By the above mentioned examples, my intention was to raise your attention, by changing the characteristics of colours and colour balance should be taken with care. We must know the consequences of our modifications, for example by increasing the contrast we have break up the colour balance of the image. It's possible to increase the contrast without breaking up the colour balance by not using the whole range of dynamic intensity range for all three colour channels.

  BINARIZATION
  A special case of intensity transformation is binarization, where the transformation degree is determined by a k threshold value (figure 60.). A special binarization transformation: slicing filter, where reseting is performed on the intensity band or within a range of an intensity band (figure 61.).
  Figure 60. Binarization reset the intensity value under k threshold, and set the value to 1 for those having intensity value over k threshold. T describes the intensity transformation function, in our case its a step-function.
  Figure 61. Slicing filters are working inside a band by reset it's value (in other words, they subtract the intensity band from the image), or keep it.

  Chapter 11. ABSTRACT KNOWLEDGE ON DIGITAL FILTERS.
  Digital filters are usually image processing methods manipulating on frequency and time ranges. Time range have been used in early days of image processing, today they are called spatial range methods. Images within a frequency range are called spectrums.
  Let's define an image with s(t), spectrum of it is S(f) and the Fourier-transformation with F. Time and frequency are connected by Fourier-transformation:
    S(f) = F{s(t)}
  The inverse operation is:
    s(t) = F^-1{S(f)}
  The impact of digital filters are represented by functions (transfer functions) in frequency ranges (eg.: convolution filters), while others are interpreted in time ranges (eg.: median filters). Let's multiply an image's spectrum with a quadratic function defined on the range of [-f,f] and 0 apart from it, then f frequencies larger then upper bound frequencies are removed (smoothened). The smoothened image is the result of the inverse Fourier-transformation applied on the altered spectrum. The scale of smoothness depends on the upper frequency bound, the lower value set on upper bound, the rougher smoothness will be resulted.
  Generally this is how digital filters operates:
    - let there be an s(t) function (which is the digital image itself)
    - apply Fourier-transformation or calculate it's spectrum: S(f) = F{s(t)}
    - multiply the spectrum with transfer function: S'(f) = S(f) A(f), where A(f) transfer function (quadratic function was used in the example)
    - apply inverse Fourier-transformation on new spectrum data at the end a filtered image is created: s'(t) = F^-1{S'(f)}

  Most of the "well-behaved" functions are Fourier-transformations. By knowing the transfer function in advance (defined by us, according to our needs with the image), the inverse Fourier-transformation can always be calculated. According to convolution identity, the convolution of two functions in given time range, is equal to the spectrum product of these functions:
  s(t) * a(t) = F^-1{S(f) A(f)}

  Applying the convolution identity, with given transfer function on Fourier-transformation, filtering can be performed in time range by convoluting the original image (in timerange) with the given transfer function Fourier-transformation:
  s'(t) = s(t) * F^-1{A(f)}, where A(f) transform function.

  Above discussed functions are continous functions. In case of discrete functions, the Fourier-transformation is simply called Discrete Fourier-transformation (DFT). The fast and efficient version of DFT is the Fast Fourier-transformation (FFT) with the following parameters:
  - s(t) digital image itself
  - A(f) a discrete transform function as well as it's inverse function.
  - a(t) sampling function
  Convulution on time range will use the results of a(t) sampling function.
  Kernel keyword have been introduced for the discrete version of transform function inverse Fourier-transformation. Kernels are widely used not only in case of the effect of function can be defined by a product of spectrum and a function, but in cases where the result of time range function can not be given within a frequency range (pl. range filters).
  Most of the digital filters are using kernels. The way these filters operate can be summarized in the following steps:
    - take a kernel with the size of [(2k+1) * (2k+1)]
    - run the given kernel window on every pixel of the image, in a way that the centre of the window is on the actual pixel.
    - calculate the filtered value of the actual pixel by the values below the kernel with the help of algorithms.
  Usually done with the following method: let g(x,y) = F{f(x,y)}, where g(x,y) is the filtered image at (x,y) coordinate, f represents the original image, F{} operator calculates the filtered value at (x,y) coordinate of the original image with the use of (x,y) neighbours intensity value. Taking advantage of observations, that intensity values of nearby points are closely related, than distant points. Another important property of these filters is that they are not recursively applyable, meaning that the procedure rely on the intensity values of the original image, namely data will always be taken from the original image.

  Chapter 12. DIGITAL FILTERS
  
  CONVOLUTION FILTERS
  Let f_1(t) a convolution function and f_2(t) the kernel. By convoluting the values in the kernel to time functions, which is the convoluted value at t time, this is how convolution or linear filters operates. 
  #Formula
  As for two-dimensional cases, for example a digital image the given formula will be changed as:
  #Formula
  Figure 62. A square impulse is the transfer function of the low-pass filter within a given range (for simplicity we are using a one-dimensional time function.

  Values within the kernel determines the characteristics of filter's transfer function, which are actually the inverz Fourier-transformation with discrete values on transfer function.
  Figure 63. Converse Fourier-transformation of the square impulse function within a sinc time interval.

  For example let's take a closer look at a low-pass filter with f_f upper bound frequency. The transfer function is shown on figure 62 and it's converse Fourier-transformation shown on figure 63 with respectively sampled kernel data coeficcients. Applying convolution on given kernel data will resulting convolution filters. Transform characteristic depends on kernel data (which is the transform function on given frequency interval). In two-dimensional data like a digital image, the kernel is a 2D sinc function, described on figure 64. figure 65. describes the movement of kernel over a digital image.
  Figure 64. A two-dimensional sinc function describing an anti-aliasing filter's kernel based on sampled data.
  Figure 65. Magnified kernel moving across a LANDSAT image.

  SEPARABLE FILTERS
  By altering the shape of the kernel to a special form, calculation time of convolution may increase rapidly. Let's split up a "w" kernel matrix into a product of a column and a row vector, namely w(i,y) = u(i) * v(j), then the convolution of image with u then with v:
  #Formula
  resulting
  #Formula

  BOX FILTER
  It's a special kernel based filter, where kernel data are having the same value, namely pixel values under the kernel will be averaged. Result of a box filter is a smoothened image. Applied the filter by itself does not give good results, however combining with other filters will be very effective tool. Due to it's simplicity, it's quite a popular filter, despect of it's unfavourable effect on spectrum. If you think about it thoroughly, convolution have been carried out by a square function on a time interval with a sinc function on a frequency range as a transform function, where upper limit frequency can be interpreted.

  GAUSS FILTER
  Values inside a Gauss-filters kernel are the two dimensional Gauss distribution values. Gauss distribution can be calculated as:
  #Formula
  Expected 0 value, sigma distributive Gauss-curve, from which the kernel can be calculated by applying data sampling on G function over grid points.
  #Formula
  Therefore the Gauss filter is separable, u and v vectors are calculated:
  #Formula
  
  By applying the second power on values of the bell-shaped curve, we do not need to multiply during convolution, but bit shifting can be applied. This is a faster implementation for Gaus filter. Consequently u and v vector are change as the following:
  #Formula

  As we have seen, Gaus filter has a smoothening characteristics. The deegree of smoothness greatly depend on the magnitude of distribution, however with high distribution values, kernel size has to be increased according to it. Gauss filters are not really usefull by itself, it should be combined with other algorithms like Canny edge detection. It's transform method is not the best, although it's still better than a box filter's. Chunk methods are usually used, cutting the length of up-bound kernel, therefore improving run time qualities. (Do not forget, that a sinc function have been sampled in this case)
  Figure 66. From left to right: Original image. A 3x3 sized kernel, Gauss filter with signma=1.0. Filtered image with 7x7 kernel box-filter. Filtered image with 7x7 kernel Gauss filter by sigma 2.0.

  One of it's disadvantage is that edges are blurred away lossing their contrasts on upper-bound frequency methods. It's not surprising if we think about it, due to the fact that high frequency values have been removed from the spectrum, as edges are represented by high frequency values.
  Physical size of images can be reduced ( by half the size). The algorithm: applying Gauss filter on the original image, after that remove every second row and columns, resulting Gauss pyramid. A Gauss pyramid can be used at texture detection applications, as the algorithm emphasizes heterogenous parts of the image by subtracting the images above each other.

  NON-SEPARABLE FILTERS
  Non-separable filters are those, which kernel matrix can not be defined by the product of two vectors. We have to apply summation plays in the convolution formula in these cases.

  Chapter 13.
  EDGE DETECTION
  Edge detectors are local based filters, on a given point's with the help of neighbours derivative intesity values (figure 67.). Our goal is to emphasize the edges, pixels having similar properties shall be removed.
  Figure 67. Maximum value on first derivative over the edges, second derivative has a zero value.

  GRADIENT FILTER
  By using the image as surface points, taking the derivative with $x$ and $y$ gradient vector, approximating the differential coefficient.
  #Formula

  The filter reacts to large intensity changes, resulting 0 results for homogenous areas. It's kernel is defined in the following:
  #Formula
  #Formula
  The "p" value may freely selected, usually used values are p=2, p=3, p=2+sqrt(2). With p=2 it's a Prewitt operator, p=3 it's a Sobel operator and p=2+sqrt(2) is an izotropical operator (figure 68). The result must be normalized with p.
  Figure 68. From left to right. Original image. An x and y vector graident approximation with izotropical gradient filter. Edge emphasize from previous two images.

  LAPLACE FILTER
  Operator definition of a Laplac filter:
  #Formula

  The following equation is a result of a secondary derivative at (x,y) point if the Laplace operator have been discretized.
  #Formula

  Therefore the Laplace filtered value at (x,y) point can be calculated with the following convolution kernel:
  #Formula
  Applications of a Laplace filter are: difference between a smoothened and original image, it reacts very efficiently to large intensity value fluctuations, therefore it reacts well for errors as well. Without smoothning, it's not effective at all.
  Figure 69. From left to right. The original image. Laplace filtered image. LoG filtered image. Emboss filtered image set for detection of edges having North-East to South-West directions.

  LoG FILTER
  We may achieve a good edge detection filter by applying a Gauss smoothening before a Laplace filter (figure 69). Due to the fact that convolution is an associative operation, there fore it is possible to convolate the kernels of Laplace and Gauss filter and use the resulted matrix. This type of filter is called "Laplacian of Gaussian" (LoG). LoG reacts sensitively to noises and good for edge detection. It's kernel matrix look like the following:
  #Formula

  EMBOSS FILTER
  Goal of Emboss filters are detecting edges in different directions (figure 69). A special kernel have been used where on the opposite corners of the matrix +1 and -1 are used. The filter reacts sensitively to edges perpendicular the values on the diagonals. An example for the kernel of an emboss filter:
  #Formula
  The kernel will detect North-East to South-West direction edges, while edges perpendicular to it will be neglected.

  CANNY EDGE DETECTOR
  Edges detection plays an important role in object recognition and vectorization of raster maps. Edges are the places on a map, where change on intensity value is the greatest. First of all, let's decide how sensitive should it be. Its worthy to apply smoothening filters or median filters to avoid every unimportant edges. A well-known method for smoothening is the application of Gauss function convolution. Let h be convolution of function f and g. It can be derive that:
  #Formula
  namely a derivative of the convolutionan on an image (be f) with Gauss-function (be g) is equivalent with the convolution of an image and the derivative of the Gauss function. Based on this formula, the concept of edge detection looks like the following:
  - Convolute f with g'
  - Calculate the absolute value of h'
  - Define places for edges, where the gradient value of h exceeds a predefined threshold value.
  Canny edge detector is not sensitive on direction of edges, therefore every edges are detected (an edges described as a line). Let's take a closer look at it's operation:
  #Formula
  where (fx,fy) is the gradient at point (x,y),
  #Formula
  where M(x,y) is the strength at point (x,y)
  #Formula
  as well as the derivative of the direction vector at point (x,y), which is the normal vector of the edge.

  The fx and fy values can be approximated by calculating the x and y direciton izotropical convoluted value with gradient filter at given point, then calculate the M(x,y) value from it. Edges are visible on this image, however every edge is many pixel thich, since edges are not always clearly visible on an image. With this method, a perfect edges, may be thicker than 0 pixel value.

  To solve this issue, a non-maximal edge elimination is introduced. This can be carried out by copying M(x,y) to an output image, with a given angle specified by theta(x,y) normal vector step in both direction on M(x,y) image, if any of the intensity value is greater than the actual, remove it the pixel from the output image. As the result of a non-maximal edges elimination process an output image is created containing every edges, displayed with a single line.
  Figure 70. From left to right. 1. Original image. 2-3 Approximation with x and y direction graident filter. 4. Edge detection from two previous images.

  Since every edges, even the lightest ones are presented on the output image, we may need to remove the lighter and keep the stronger, global edges. By applying an elimination process, continuity of edges are not taken into consideration, whereas we do not want break on the edges. The solution is edge thresholding. The basic idea behind it is to eliminate all points below a threshold value, and keep all the point over that threshold value to get the right pixel values between threshold value. The best approach would be a variation of th depth-first-search algorithm, in addition with the condition: when arriving to a definately good point, points lying on the path shall be kept (figure 70-71).
  Figure 71. From left to right: 1: Edge expansion image 2: Elimination of non-maximum edges. 3-4: Edges threshold, with 10,40 and 40,100 parameters. On the second case only the strongest edges remained.

  Canny filter can be applied for cases, where additive and Gauss-typed noise are present on an image. A simple approximation method, By firstly apply Gauss-smoothening then apply gradient mask. Since both filters are linear, therefore filtering can be done in one step, as described in filter concept section.

  IMAGE SHARPENING
  The above mentioned filters are capable of sharpening and improving the quality of images. The basic idea behind them is by adding the difference between the original and smoothened image to the original image:
  g(x,y) = f(x,y) + [f(x,y) - fs(x,y)]
  where fs is the smoothened image.

  CHAPTER 14.
  NON-LINEAR FILTERS
  Non-linear filters are procedures, calculating the filtered value from a given pixel's neighbours, but not as the linear combination of the neighbour pixel values. We can no longer talk about kernel matrixes, only kernels or kernel windows.

  RANK FILTER
  The basic idea behing rank filters is to use the intensity values of pixels under a kernel window, namely sort them in order, then by this order select new intensity value for filtering pixel. One of the most widely used rank filter is the median filter, where the middle value is chosen in the sorted intensity list as the filtering pixel value (figure 72).
  The result of filtering is a smoothened image, but transfer function can not assigned. Local noises are filtered very efficiently. "Salt and pepper" errors (small value, point based fluctuations) are successfully removed, because by ordering of intesity values, those highly different values (dark or light) pixels are sorted to the edges. Figure 73. describes a median filter applied on a noisy curve.
  Figure 72. A noisy curve (with dashed lines) and median filtered variation of if (continous curve). Length of the kernel is "a".

  As for digital images, an important attribute for median filter having kernel size of 2k+1, lines thinner than k are eliminated from the image. It's usefull when emphasizing large areas. Unfortunately it may shift the edges, round off corners, however the algorithm may be altered to eliminate these defects.
  Figure 73. From left to right: Original image, "salt and pepper" errors. Conservatively smoothened image removing errors. 5x5 median filtered image. 11x11 median filtered image.

  OLYMPIC FILTER
  Similarly to the median filter, high intensity values are considered as noise. Certain sports are following the scoring system of the olympics. Values are sorted under the kernel window, then eliminating all candidates largely different from the median. It can be parameterized that how many elements shall be ignored from the largest and smallest elements.

  CONSERVATIVE SMOOTHENING
  The conservative smoothening noise filtering method, mostly suitable for eliminating "salt and pepper" noises. It's strategy is to sort all the pixels within the kernel window in increasing order except the actual pixel. We get a [min..max] interval, then check if the actual pixel is inside this interval or not (figure 73). If it's inside the interval, then the intensity value of the actual pixel is not changed, otherwise if it's greater than the maximum, then it's value will be the max. value, similarly to the minimal case.

  KUWAHARA FILTER
  The Kuwahara-filter is a noise and smoothening filter. One of it's most important attribute is that it does not shift or blur the edges way. Let's split up the kernel window into four overlapping (k+1)x(k+1) sized squares, starting from four corners (figure 74.) The filtered pixel's value will be the average intensity value of the current square, having the smallest standard deviation.
  Figure 74. Window arrangement of a Kuwahara filter and the behaviour of the kernel over a corner.
  Letters identifying, which square does a pixel belongs to. The middle pixel is contained by every squares. After sorting all the pixels to the right squares, the average pixel value and empirical distribution value is calculated for each square. After that, the average value is given to the pixel having the smallest standard deviation (figure 75).
  Figure 75. From left to right: Original image. A 5x5 Kuwahara filtered image. A 11x11 Kuwahara filtered image. The filtered image refiltered with a 5x5 Kuwahara filter.


  CHAPTER 15.
  THRESHOLDING
























